{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SediNet_Continuous_Sand_9Prcs.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Mmv6E4xVrPKH","colab_type":"text"},"source":["## Sedinet: predict 9 percentiles of the grain size distribution from generic images of sands"]},{"cell_type":"markdown","metadata":{"id":"9q6xzJ6hrUSP","colab_type":"text"},"source":["This Jupyter notebook accompanies the [SediNet](https://github.com/MARDAScience/SediNet) package\n","\n","Written by Daniel Buscombe, MARDA Science\n","\n","daniel@mardascience.com\n","\n","\n","> Demonstration of how to use SediNet to estimate from an ensemble of three models to estimate nine percentiles of the cumulative grain size distribution from a images of sands\n","\n","First, this notebbok assumes you are a cloud computer such as Colab so we first download the SediNet package from github:\n"]},{"cell_type":"code","metadata":{"id":"gHs5dUy8sxM-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":162},"executionInfo":{"status":"ok","timestamp":1593216178181,"user_tz":420,"elapsed":90251,"user":{"displayName":"Daniel Buscombe","photoUrl":"","userId":"01832231008732716345"}},"outputId":"7a6ce723-2f8b-42e7-8f68-838d2ae42a75"},"source":["!git clone --depth 1 https://github.com/MARDAScience/SediNet.git"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Cloning into 'SediNet'...\n","remote: Enumerating objects: 760, done.\u001b[K\n","remote: Counting objects: 100% (760/760), done.\u001b[K\n","remote: Compressing objects: 100% (688/688), done.\u001b[K\n","remote: Total 760 (delta 87), reused 716 (delta 68), pack-reused 0\u001b[K\n","Receiving objects: 100% (760/760), 1.11 GiB | 15.88 MiB/s, done.\n","Resolving deltas: 100% (87/87), done.\n","Checking out files: 100% (725/725), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"91QV0zqztBZD","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593216178189,"user_tz":420,"elapsed":90254,"user":{"displayName":"Daniel Buscombe","photoUrl":"","userId":"01832231008732716345"}}},"source":["import os, json\n","os.chdir('SediNet')"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"1oi9C5Hcqsjh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1593216180628,"user_tz":420,"elapsed":2427,"user":{"displayName":"Daniel Buscombe","photoUrl":"","userId":"01832231008732716345"}},"outputId":"33407865-e48c-44c9-c1d8-ef648395d0c9"},"source":["import tensorflow as tf\n","print(tf.__version__)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["2.2.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Wr9_1aJ6ozwP","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593216180974,"user_tz":420,"elapsed":2762,"user":{"displayName":"Daniel Buscombe","photoUrl":"","userId":"01832231008732716345"}}},"source":["from sedinet_eval import *\n","from numpy import any as npany"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZftisMCWrCxy","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593216180977,"user_tz":420,"elapsed":2758,"user":{"displayName":"Daniel Buscombe","photoUrl":"","userId":"01832231008732716345"}}},"source":["configfile = 'config/config_sand_predict.json'\n","weights_path = 'grain_size_sand_generic/res/grey/sand_generic_9prcs_simo_batch8_P10_P16_P25_P5_P50_P75_P84_P90_P95__checkpoint.hdf5'"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I_opHDLcrLpW","colab_type":"text"},"source":["Load the config file and parse out the variables we need"]},{"cell_type":"code","metadata":{"id":"GjskF4hBrL1C","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593216180980,"user_tz":420,"elapsed":2758,"user":{"displayName":"Daniel Buscombe","photoUrl":"","userId":"01832231008732716345"}}},"source":["try:\n","    # load the user configs\n","    with open(os.getcwd()+os.sep+configfile) as f:\n","       config = json.load(f)\n","except:\n","    # load the user configs\n","    with open(configfile) as f:\n","       config = json.load(f)    \n","    \n","###===================================================\n","## user defined variables: proportion of data to use for training (a.k.a. the \"train/test split\")\n","csvfile = config[\"csvfile\"] #csvfile containing image names and class values\n","res_folder = config[\"res_folder\"] #folder containing csv file and that will contain model outputs\n","name = config[\"name\"] #name prefix for output files\n","greyscale = config[\"greyscale\"] #convert imagery to greyscale or not\n","dropout = config[\"dropout\"] \n","\n","try:\n","   numclass = config['numclass']\n","except:\n","   numclass = 0\n","        \n","vars = [k for k in config.keys() if not npany([k.startswith('csvfile'), k.startswith('dropout'), k.startswith('base'), k.startswith('res_folder'), k.startswith('train_csvfile'), k.startswith('test_csvfile'), k.startswith('name'), k.startswith('greyscale'), k.startswith('aux_in'), k.startswith('N'), k.startswith('numclass')])]\n","\n","vars = sorted(vars)\n","\n","csvfile = res_folder+os.sep+csvfile"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xht-WH_4rTqZ","colab_type":"text"},"source":["This next part reads the data in from the csv file as a pandas dataframe, gets an image generator, and then prepares three models with different base values"]},{"cell_type":"code","metadata":{"id":"WuLSL_Narm41","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":146},"executionInfo":{"status":"ok","timestamp":1593216182140,"user_tz":420,"elapsed":3910,"user":{"displayName":"Daniel Buscombe","photoUrl":"","userId":"01832231008732716345"}},"outputId":"ef5aa8f5-17ac-44f8-86ad-934804fb7231"},"source":["df = pd.read_csv(csvfile)\n","df['files'] = [k.strip() for k in df['files']]\n","\n","train_idx = np.arange(len(df))\n","\n","CS = joblib.load(weights_path.replace('.hdf5','_scaler.pkl')) \n","\n","varstring = ''.join([str(k)+'_' for k in vars])\n","      \n","##==============================================\n","## create a sedinet model to estimate category\n","model = make_sedinet_siso_simo(vars, greyscale, dropout) \n","model.load_weights(os.getcwd()+os.sep+weights_path)\n","\n","Z = joblib.load(weights_path.replace('.hdf5','_bias.pkl'))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.preprocessing.data module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.preprocessing. Anything that cannot be imported from sklearn.preprocessing is now part of the private API.\n","  warnings.warn(message, FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator RobustScaler from version 0.21.3 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n","  UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["==========================================\n","[INFORMATION] Model summary:\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"A-UVpFDG6xIR","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593216219479,"user_tz":420,"elapsed":41246,"user":{"displayName":"Daniel Buscombe","photoUrl":"","userId":"01832231008732716345"}}},"source":["train_gen = get_data_generator_Nvars_siso_simo(df, train_idx, False, vars, len(df), greyscale, CS)\n","\n","#df['files'] = [os.getcwd()+os.sep+f.replace('\\\\',os.sep) for f in df['files']]\n","#train_gen = get_data_generator_Nvars_siso_simo(df, train_idx, False, vars, len(df), greyscale, CS) #   \n","#x_train, tmp = next(train_gen)\n","\n","x_train, tmp = next(train_gen)\n","if len(vars)>1:\n","   counter = 0\n","   for v in vars:\n","      exec(v+'_trueT = np.squeeze(CS[counter].inverse_transform(tmp[counter].reshape(-1,1)))')\n","      counter +=1\n","else:\n","   exec(vars[0]+'_trueT = np.squeeze(CS[0].inverse_transform(tmp[0].reshape(-1,1)))')\n","         \n","if len(vars)>1:\n","   counter = 0\n","   for v in vars:\n","      exec(v+'_trueT = np.squeeze(CS[counter].inverse_transform(tmp[counter].reshape(-1,1)))')\n","      counter +=1\n","else:\n","   exec(vars[0]+'_trueT = np.squeeze(CS[0].inverse_transform(tmp[0].reshape(-1,1)))')"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dwz1HlwcsJj4","colab_type":"text"},"source":["Now the models are set up, we use them below to make predictions on each image so we end up with three estimates per image, and our final estimate is their mean"]},{"cell_type":"code","metadata":{"id":"VCphNJ-JsJux","colab_type":"code","colab":{}},"source":["for v in vars:\n","    exec(v+'_PT = []')\n","\n","tmp = model.predict(x_train, batch_size=1)\n","\n","if len(vars)>1:\n","    counter = 0\n","    for v in vars:\n","       exec(v+'_PT.append(np.squeeze(CS[counter].inverse_transform(tmp[counter].reshape(-1,1))))')\n","       counter +=1\n","else:\n","    exec(vars[0]+'_PT.append(np.asarray(np.squeeze(CS[0].inverse_transform(tmp.reshape(-1,1)))))') "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cQSAxwespJai","colab_type":"code","colab":{}},"source":["if len(vars)>1:\n","    for k in range(len(vars)):\n","       exec(vars[k]+'_predT = np.squeeze(np.mean(np.asarray('+vars[k]+'_PT), axis=0))')\n","else:\n","    exec(vars[0]+'_predT = np.squeeze(np.mean(np.asarray('+vars[0]+'_PT), axis=0))')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rww4YXcysFAz","colab_type":"code","colab":{}},"source":["if len(vars)==9:\n","   nrows = 3; ncols = 3\n","elif len(vars)==8:\n","   nrows = 4; ncols = 2\n","elif len(vars)==7:\n","   nrows = 3; ncols = 3\n","elif len(vars)==6:\n","   nrows = 3; ncols = 2\n","elif len(vars)==5:\n","   nrows = 3; ncols = 2\n","elif len(vars)==4:\n","   nrows = 2; ncols = 2\n","elif len(vars)==3:\n","   nrows = 1; ncols = 3\n","elif len(vars)==2:\n","   nrows = 2; ncols = 1\n","elif len(vars)==1:\n","   nrows = 1; ncols = 1\n","\n","## make a plot\n","fig = plt.figure()\n","labs = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n","for k in range(1,1+(nrows*ncols)):\n","   #try:\n","   plt.subplot(nrows,ncols,k)\n","   x = eval(vars[k-1]+'_trueT')\n","   y = eval(vars[k-1]+'_predT')\n","   y = np.polyval(Z[k-1],y) #apply bias correction\n","   y = np.abs(y) #make sure no negative values\n","\n","   plt.plot(x, y, 'ko', markersize=5)\n","\n","   plt.plot([ np.min(np.hstack((x,y))),  np.max(np.hstack((x,y)))], [ np.min(np.hstack((x,y))),  np.max(np.hstack((x,y)))], 'k', lw=2)\n","\n","   plt.text(1.02*np.min(np.hstack((x,y))), 0.98*np.max(np.hstack((x,y))),'Test : '+str(np.mean(100*(np.abs(eval(vars[k-1]+'_predT') - eval(vars[k-1]+'_trueT')) / eval(vars[k-1]+'_trueT'))))[:5]+' %', fontsize=8)\n","\n","   plt.title(r''+labs[k-1]+') '+vars[k-1], fontsize=8, loc='left')\n","   plt.xlabel('Actual '+vars[k-1])\n","   plt.ylabel('Estimated '+vars[k-1])\n","      \n","   plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Nwqco8Mpgq5","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}