{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SediNet_Categorical_Pop.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Mmv6E4xVrPKH","colab_type":"text"},"source":["## Sedinet: predict categorical population"]},{"cell_type":"markdown","metadata":{"id":"9q6xzJ6hrUSP","colab_type":"text"},"source":["This Jupyter notebook accompanies the [SediNet](https://github.com/MARDAScience/SediNet) package\n","\n","Written by Daniel Buscombe, MARDA Science\n","\n","daniel@mardascience.com\n","\n","\n","> Demonstration of how to use SediNet to estimate from an ensemble of three models to estimate sediment population\n","\n","First, this notebbok assumes you are a cloud computer such as Colab so we first download the SediNet package from github:\n"]},{"cell_type":"code","metadata":{"id":"gHs5dUy8sxM-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":162},"executionInfo":{"status":"ok","timestamp":1593213049651,"user_tz":420,"elapsed":44864,"user":{"displayName":"Daniel Buscombe","photoUrl":"","userId":"01832231008732716345"}},"outputId":"282da0e4-c6d7-47ea-e375-adc96bed5be1"},"source":["!git clone --depth 1 https://github.com/MARDAScience/SediNet.git"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Cloning into 'SediNet'...\n","remote: Enumerating objects: 760, done.\u001b[K\n","remote: Counting objects: 100% (760/760), done.\u001b[K\n","remote: Compressing objects: 100% (688/688), done.\u001b[K\n","remote: Total 760 (delta 87), reused 716 (delta 68), pack-reused 0\u001b[K\n","Receiving objects: 100% (760/760), 1.11 GiB | 46.80 MiB/s, done.\n","Resolving deltas: 100% (87/87), done.\n","Checking out files: 100% (725/725), done.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"604kcwaVRbx4","colab_type":"text"},"source":["Load TF v 2"]},{"cell_type":"code","metadata":{"id":"Ucd0f__hRWLv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":128},"executionInfo":{"status":"ok","timestamp":1593213051519,"user_tz":420,"elapsed":38792,"user":{"displayName":"Daniel Buscombe","photoUrl":"","userId":"01832231008732716345"}},"outputId":"42363b88-c25c-4bb2-d37d-f5cfb9716f9e"},"source":["import tensorflow as tf\n","print(tf.__version__)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["2.2.0\n","WARNING:tensorflow:From <ipython-input-2-27f16c16655a>:4: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.config.list_physical_devices('GPU')` instead.\n","False\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"91QV0zqztBZD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1593213095816,"user_tz":420,"elapsed":308,"user":{"displayName":"Daniel Buscombe","photoUrl":"","userId":"01832231008732716345"}},"outputId":"6fbd4a01-00e5-47a3-bf9d-ffc97cdbfe67"},"source":["import os, json\n","os.getcwd()"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic":{"type":"string"},"text/plain":["'/content/SediNet'"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"3bwKqnDx5kb3","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593213114748,"user_tz":420,"elapsed":319,"user":{"displayName":"Daniel Buscombe","photoUrl":"","userId":"01832231008732716345"}}},"source":["os.chdir('SediNet')"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I9YJKEa3sxVG","colab_type":"text"},"source":["Import everything we need from sedinet_models.py"]},{"cell_type":"code","metadata":{"id":"1oi9C5Hcqsjh","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593213115558,"user_tz":420,"elapsed":253,"user":{"displayName":"Daniel Buscombe","photoUrl":"","userId":"01832231008732716345"}}},"source":["from sedinet_eval import *\n","from numpy import any as npany"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZftisMCWrCxy","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593213116988,"user_tz":420,"elapsed":234,"user":{"displayName":"Daniel Buscombe","photoUrl":"","userId":"01832231008732716345"}}},"source":["configfile = 'config_pop_predict.json'\n","\n","weights_path = 'grain_population/res/color/pop_model_checkpoint.hdf5'"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I_opHDLcrLpW","colab_type":"text"},"source":["Load the config file and parse out the variables we need"]},{"cell_type":"code","metadata":{"id":"GjskF4hBrL1C","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593213118561,"user_tz":420,"elapsed":288,"user":{"displayName":"Daniel Buscombe","photoUrl":"","userId":"01832231008732716345"}}},"source":["# load the user configs\n","with open(os.getcwd()+os.sep+'config'+os.sep+configfile) as f:    \n","  config = json.load(f)     \n","\n","###===================================================\n","## user defined variables: proportion of data to use for training (a.k.a. the \"train/test split\")\n","csvfile = config[\"csvfile\"] #csvfile containing image names and class values\n","res_folder = config[\"res_folder\"] #folder containing csv file and that will contain model outputs\n","name = config[\"name\"] #name prefix for output files\n","greyscale = config[\"greyscale\"] #convert imagery to greyscale or not\n","dropout = config[\"dropout\"] #dropout factor\n","    \n","try:\n","   numclass = config['numclass']\n","except:\n","   numclass = 0\n","                        \n","#output variables            \n","vars = [k for k in config.keys() if not npany([k.startswith('base'), k.startswith('csvfile'), k.startswith('res_folder'), k.startswith('train_csvfile'), k.startswith('test_csvfile'), k.startswith('name'), k.startswith('greyscale'), k.startswith('aux_in'), k.startswith('dropout'), k.startswith('N'), k.startswith('numclass')])]\n","vars = sorted(vars)\n","\n","###==================================================\n","ID_MAP = dict(zip(np.arange(numclass), [str(k) for k in range(numclass)]))\n","\n","csvfile = res_folder+os.sep+csvfile"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"7BS3QKVeS1SG","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593213119538,"user_tz":420,"elapsed":288,"user":{"displayName":"Daniel Buscombe","photoUrl":"","userId":"01832231008732716345"}}},"source":["if len(vars) ==1:\n","   mode = 'siso'\n","elif len(vars) >1:\n","   mode = 'simo'"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FL-FdevJSvaT","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"xht-WH_4rTqZ","colab_type":"text"},"source":["This next part reads the data in from the csv file as a pandas dataframe, gets an image generator, and then prepares a model"]},{"cell_type":"code","metadata":{"id":"paMbpulBUnLh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":635},"executionInfo":{"status":"ok","timestamp":1593213122042,"user_tz":420,"elapsed":573,"user":{"displayName":"Daniel Buscombe","photoUrl":"","userId":"01832231008732716345"}},"outputId":"f48e7ed8-12ab-4128-9006-89877f62b260"},"source":["ID_MAP = dict(zip(np.arange(numclass), [str(k) for k in range(numclass)]))\n","   \n","###===================================================\n","## read the data set in, clean and modify the pathnames so they are absolute\n","df = pd.read_csv(csvfile)\n","df['files'] = [k.strip() for k in df['files']]\n","df['files'] = [os.getcwd()+os.sep+f.replace('\\\\',os.sep) for f in df['files']]\n","\n","train_idx = np.arange(len(df))\n","\n","train_gen = get_data_generator_1vars(df, train_idx, True, vars, greyscale, len(df))\n","   \n","##==============================================\n","## create a SediNet model to estimate sediment category\n","SM = make_cat_sedinet(ID_MAP, dropout)\n","SM.load_weights(os.getcwd()+os.sep+weights_path)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["==========================================\n","[INFORMATION] Model summary:\n","Model: \"model\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         [(None, 1024, 1024, 3)]   0         \n","_________________________________________________________________\n","conv2d (Conv2D)              (None, 1022, 1022, 30)    840       \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 1020, 1020, 60)    16260     \n","_________________________________________________________________\n","max_pooling2d (MaxPooling2D) (None, 510, 510, 60)      0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 508, 508, 90)      48690     \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 254, 254, 90)      0         \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 252, 252, 120)     97320     \n","_________________________________________________________________\n","max_pooling2d_2 (MaxPooling2 (None, 126, 126, 120)     0         \n","_________________________________________________________________\n","global_max_pooling2d (Global (None, 120)               0         \n","_________________________________________________________________\n","dropout (Dropout)            (None, 120)               0         \n","_________________________________________________________________\n","dense (Dense)                (None, 128)               15488     \n","_________________________________________________________________\n","output (Dense)               (None, 6)                 774       \n","=================================================================\n","Total params: 179,372\n","Trainable params: 179,372\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"n_uakpvLUp3f","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1593213122829,"user_tz":420,"elapsed":244,"user":{"displayName":"Daniel Buscombe","photoUrl":"","userId":"01832231008732716345"}},"outputId":"3c0b4a7a-e1d5-4c88-a5a8-b7c2d427829c"},"source":["vars"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['pop']"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"Dwz1HlwcsJj4","colab_type":"text"},"source":["Now the models are set up, we use them below to make predictions on each image so we end up with three estimates per image, and our final estimate is their mode\n","\n","A classification report is printed to screen showing per-class F1 scores which is an average of precision and recall. Precision is the proportion of positive identifications that are correct (a precision of 1 means there are no false positives), and recall is the proportion of actual positives identified correctly (a recall of 1 means there are no false negatives). "]},{"cell_type":"code","metadata":{"id":"VCphNJ-JsJux","colab_type":"code","colab":{}},"source":["x_train, (trueT)= next(train_gen)\n","trueT = trueT[0] \n","\n","predT = SM.predict(x_train, batch_size=1)\n","   \n","del x_train, train_gen\n","   \n","predT = np.asarray(predT).argmax(axis=-1)\n","\n","## print a classification report to screen, showing f1, precision, recall and accuracy\n","print(\"==========================================\")\n","print(\"Classification report for \"+vars[0])\n","print(classification_report(trueT, predT))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AAmUYcofsE3E","colab_type":"text"},"source":["Finally we print a confusion matrix showing normalized  correspondences between actual and estimated labels"]},{"cell_type":"code","metadata":{"id":"rww4YXcysFAz","colab_type":"code","colab":{}},"source":["classes = np.arange(len(ID_MAP))\n","##==============================================\n","## create figures showing confusion matrices for data set\n","plot_confmat(predT, trueT, vars[0]+'T',classes)  \n","plt.savefig(weights_path.replace('.hdf5','_cm_predict.png'), dpi=300, bbox_inches='tight') \n","plt.close('all')   "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-4eQndk2guty","colab_type":"text"},"source":["See `pop_model_checkpoint_cm_predict.png` inside grain_population/res/color/"]},{"cell_type":"code","metadata":{"id":"dBGWBO330EvN","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}