{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mmv6E4xVrPKH"
   },
   "source": [
    "## Sedinet: predict sieve size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9q6xzJ6hrUSP"
   },
   "source": [
    "This Jupyter notebook accompanies the [SediNet](https://github.com/MARDAScience/SediNet) package\n",
    "\n",
    "Written by Daniel Buscombe, MARDA Science\n",
    "\n",
    "daniel@mardascience.com\n",
    "\n",
    "\n",
    "> Demonstration of how to use SediNet to estimate from an ensemble of three models to estimate equivalent sieve size, from a set of images of sieved sediment fractions\n",
    "\n",
    "First, this notebbok assumes you are a cloud computer such as Colab so we first download the SediNet package from github:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1423,
     "status": "ok",
     "timestamp": 1564421877217,
     "user": {
      "displayName": "Daniel Buscombe",
      "photoUrl": "",
      "userId": "01832231008732716345"
     },
     "user_tz": 420
    },
    "id": "gHs5dUy8sxM-",
    "outputId": "55a217ff-fba5-42b1-dc33-d499ce437db4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'SediNet' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone --depth 1 https://github.com/MARDAScience/SediNet.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "91QV0zqztBZD"
   },
   "outputs": [],
   "source": [
    "import os, json\n",
    "os.chdir('SediNet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I9YJKEa3sxVG"
   },
   "source": [
    "Import everything we need from sedinet_models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2791,
     "status": "ok",
     "timestamp": 1564423548915,
     "user": {
      "displayName": "Daniel Buscombe",
      "photoUrl": "",
      "userId": "01832231008732716345"
     },
     "user_tz": 420
    },
    "id": "1oi9C5Hcqsjh",
    "outputId": "4ad5e04a-a096-4df2-d18d-6eff62678f4c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sedinet_models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZftisMCWrCxy"
   },
   "outputs": [],
   "source": [
    "configfile = 'config_sievedsand_sieve.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I_opHDLcrLpW"
   },
   "source": [
    "Load the config file and parse out the variables we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GjskF4hBrL1C"
   },
   "outputs": [],
   "source": [
    "# load the user configs\n",
    "with open(os.getcwd()+os.sep+'config'+os.sep+configfile) as f:    \n",
    "  config = json.load(f)     \n",
    "\n",
    "###===================================================\n",
    "## user defined variables: proportion of data to use for training (a.k.a. the \"train/test split\")\n",
    "base    = int(config[\"base\"]) #minimum number of convolutions in a sedinet convolutional block\n",
    "csvfile = config[\"csvfile\"] #csvfile containing image names and class values\n",
    "res_folder = config[\"res_folder\"] #folder containing csv file and that will contain model outputs\n",
    "name = config[\"name\"] #name prefix for output files\n",
    "dropout = float(config[\"dropout\"]) \n",
    "add_bn = bool(config[\"add_bn\"]) \n",
    "\n",
    "vars = [k for k in config.keys() if not np.any([k.startswith('base'), k.startswith('res_folder'), k.startswith('csvfile'), k.startswith('name'), k.startswith('dropout'), k.startswith('add_bn')])]\n",
    "\n",
    "vars = sorted(vars)\n",
    "\n",
    "###==================================================\n",
    "\n",
    "csvfile = res_folder+os.sep+csvfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xht-WH_4rTqZ"
   },
   "source": [
    "This next part reads the data in from the csv file as a pandas dataframe, gets an image generator, and then prepares three models with different base values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WuLSL_Narm41"
   },
   "outputs": [],
   "source": [
    "###===================================================\n",
    "## read the data set in, clean and modify the pathnames so they are absolute\n",
    "df = pd.read_csv(csvfile)\n",
    "df['files'] = [k.strip() for k in df['files']]\n",
    "df['files'] = [os.getcwd()+os.sep+f.replace('\\\\',os.sep) for f in df['files']]    \n",
    "\n",
    "train_idx = np.arange(len(df))\n",
    "\n",
    "##==============================================\n",
    "## create training and testing file generators, set the weights path, plot the model, and create a callback list for model training   \n",
    "if len(vars)==1:        \n",
    "  train_gen = get_data_generator_1vars(df, train_idx, True, vars, len(df))\n",
    "elif len(vars)==2:        \n",
    "  train_gen = get_data_generator_2vars(df, train_idx, True, vars, len(df))\n",
    "elif len(vars)==3:        \n",
    "  train_gen = get_data_generator_3vars(df, train_idx, True, vars, len(df))\n",
    "elif len(vars)==4:        \n",
    "  train_gen = get_data_generator_4vars(df, train_idx, True, vars, len(df))\n",
    "elif len(vars)==5:        \n",
    "  train_gen = get_data_generator_5vars(df, train_idx, True, vars, len(df))\n",
    "elif len(vars)==6:        \n",
    "  train_gen = get_data_generator_6vars(df, train_idx, True, vars, len(df))\n",
    "elif len(vars)==7:        \n",
    "  train_gen = get_data_generator_7vars(df, train_idx, True, vars, len(df))\n",
    "elif len(vars)==8:        \n",
    "  train_gen = get_data_generator_8vars(df, train_idx, True, vars, len(df))\n",
    "elif len(vars)==9:        \n",
    "  train_gen = get_data_generator_9vars(df, train_idx, True, vars, len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7062,
     "status": "ok",
     "timestamp": 1564423704242,
     "user": {
      "displayName": "Daniel Buscombe",
      "photoUrl": "",
      "userId": "01832231008732716345"
     },
     "user_tz": 420
    },
    "id": "A-UVpFDG6xIR",
    "outputId": "b25df3ad-a87a-4805-a961-a604978a6185"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0729 18:08:22.661744 139889349470080 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0729 18:08:22.680096 139889349470080 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0729 18:08:22.685170 139889349470080 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0729 18:08:22.722175 139889349470080 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0729 18:08:22.769345 139889349470080 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0729 18:08:22.770709 139889349470080 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0729 18:08:22.832109 139889349470080 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W0729 18:08:22.936845 139889349470080 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0729 18:08:22.993663 139889349470080 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "[INFORMATION] Model summary:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 512, 1024, 1)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 510, 1022, 18)     180       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 508, 1020, 36)     5868      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 254, 510, 36)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 252, 508, 54)      17550     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 126, 254, 54)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 124, 252, 72)      35064     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 62, 126, 72)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 62, 126, 72)       288       \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_1 (Glob (None, 72)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 72)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               37376     \n",
      "_________________________________________________________________\n",
      "sieve_output (Dense)         (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 96,839\n",
      "Trainable params: 96,695\n",
      "Non-trainable params: 144\n",
      "_________________________________________________________________\n",
      "==========================================\n",
      "[INFORMATION] Model summary:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 512, 1024, 1)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 510, 1022, 20)     200       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 508, 1020, 40)     7240      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 254, 510, 40)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 252, 508, 60)      21660     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 126, 254, 60)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 124, 252, 80)      43280     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 62, 126, 80)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 62, 126, 80)       320       \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_2 (Glob (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               41472     \n",
      "_________________________________________________________________\n",
      "sieve_output (Dense)         (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 114,685\n",
      "Trainable params: 114,525\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "==========================================\n",
      "[INFORMATION] Model summary:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 512, 1024, 1)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 510, 1022, 22)     220       \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 508, 1020, 44)     8756      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 254, 510, 44)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 252, 508, 66)      26202     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 126, 254, 66)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 124, 252, 88)      52360     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 62, 126, 88)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 62, 126, 88)       352       \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_3 (Glob (None, 88)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 88)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               45568     \n",
      "_________________________________________________________________\n",
      "sieve_output (Dense)         (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 133,971\n",
      "Trainable params: 133,795\n",
      "Non-trainable params: 176\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x_train, tmp = next(train_gen)   \n",
    "if len(vars)>1:    \n",
    "  counter = 0\n",
    "  for v in vars:\n",
    "     exec(v+'_trueT = np.squeeze(tmp[counter])')\n",
    "     counter +=1\n",
    "else:\n",
    "  exec(vars[0]+'_trueT = np.squeeze(tmp)')\n",
    "\n",
    "models = []\n",
    "for base in [base-2,base,base+2]:\n",
    "  weights_path = name+\"_base\"+str(base)+\"_model_checkpoint.hdf5\"\n",
    "  ##==============================================\n",
    "  ## create a SediNet model to estimate sediment category\n",
    "  model = make_cont_sedinet(base, vars, add_bn, dropout)\n",
    "  model.load_weights(os.getcwd()+os.sep+'res'+os.sep+res_folder+os.sep+weights_path)\n",
    "  models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dwz1HlwcsJj4"
   },
   "source": [
    "Now the models are set up, we use them below to make predictions on each image so we end up with three estimates per image, and our final estimate is their mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VCphNJ-JsJux"
   },
   "outputs": [],
   "source": [
    "for v in vars:\n",
    "  exec(v+'_PT = []')\n",
    "#PT = []      \n",
    "for model in models:   \n",
    "  tmp = model.predict(x_train, batch_size=1)\n",
    "  if len(vars)>1:    \n",
    "     counter = 0\n",
    "     for v in vars:\n",
    "        exec(v+'_PT.append(np.squeeze(tmp[counter]))')\n",
    "        counter +=1\n",
    "  else:\n",
    "     exec(vars[0]+'_PT.append(np.asarray(np.squeeze(tmp)))') #.argmax(axis=-1))')\n",
    "\n",
    "if len(vars)>1:\n",
    "  for k in range(len(vars)):  \n",
    "     exec(vars[k]+'_predT = np.squeeze(np.mean(np.asarray('+vars[k]+'_PT), axis=0))')\n",
    "else:   \n",
    "  exec(vars[0]+'_predT = np.squeeze(np.mean(np.asarray('+vars[0]+'_PT), axis=0))')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AAmUYcofsE3E"
   },
   "source": [
    "Finally we print a confusion matrix showing normalized  correspondences between actual and estimated labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rww4YXcysFAz"
   },
   "outputs": [],
   "source": [
    "if len(vars)==9:    \n",
    "  nrows = 3; ncols = 3\n",
    "elif len(vars)==8:    \n",
    "  nrows = 4; ncols = 2\n",
    "elif len(vars)==7:    \n",
    "  nrows = 4; ncols = 2           \n",
    "elif len(vars)==6:    \n",
    "  nrows = 3; ncols = 2\n",
    "elif len(vars)==5:    \n",
    "  nrows = 3; ncols = 2       \n",
    "elif len(vars)==4:    \n",
    "  nrows = 2; ncols = 2       \n",
    "elif len(vars)==3:    \n",
    "  nrows = 3; ncols = 1      \n",
    "elif len(vars)==2:    \n",
    "  nrows = 2; ncols = 1      \n",
    "elif len(vars)==1:    \n",
    "  nrows = 1; ncols = 1\n",
    "\n",
    "## make a plot                  \n",
    "fig = plt.figure(figsize=(4*nrows,4*ncols))\n",
    "labs = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "for k in range(1,1+(nrows*ncols)):\n",
    "  plt.subplot(nrows,ncols,k)\n",
    "  plt.plot(eval(vars[k-1]+'_trueT'), eval(vars[k-1]+'_predT'), 'ko', markersize=3)\n",
    "  #plt.plot(eval(vars[k-1]+'_true'), eval(vars[k-1]+'_pred'), 'bx', markersize=5)\n",
    "  plt.plot([5, 1000], [5, 1000], 'k', lw=2)\n",
    "  plt.xscale('log'); plt.yscale('log')\n",
    "  #plt.text(11,700,'Test : '+str(np.mean(100*(np.abs(eval(vars[k-1]+'_pred') - eval(vars[k-1]+'_true')) / eval(vars[k-1]+'_true'))))[:5]+' %',  fontsize=8, color='b')\n",
    "  plt.text(11,1000,'Train : '+str(np.mean(100*(np.abs(eval(vars[k-1]+'_predT') - eval(vars[k-1]+'_trueT')) / eval(vars[k-1]+'_trueT'))))[:5]+' %', fontsize=8)\n",
    "  plt.xlim(10,1300); plt.ylim(10,1300)\n",
    "  plt.title(r''+labs[k-1]+') '+vars[k-1], fontsize=8, loc='left')\n",
    "\n",
    "#plt.show()\n",
    "plt.savefig(name+str(IM_HEIGHT)+'_batch'+str(batch_size)+'_xy-base'+str(base)+'_predict.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "del fig  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8lggiKFIvfGY"
   },
   "source": [
    "If you are in Google Colab, to view the file you just made, go to tn the File tab over on the left, expand SediNet, and you'll see the png file called \"sievesand_xxxx-predict.png\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SediNet_Continuous_Sieve.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
